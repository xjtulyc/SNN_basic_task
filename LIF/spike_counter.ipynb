{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
    "events, target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time_window\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
    "                                      transforms.ToFrame(sensor_size=sensor_size,\n",
    "                                                         time_window=1000)\n",
    "                                     ])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
    "cached_dataloader = DataLoader(cached_trainset)\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_batched():\n",
    "    events, target = next(iter(cached_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = tonic.transforms.Compose([torch.from_numpy,\n",
    "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n",
    "\n",
    "# no augmentations for the testset\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n",
    "\n",
    "batch_size = 1\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
    "testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(2, 12, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Conv2d(12, 32, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(32*5*5, 10),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, we won't return membrane as we don't need it\n",
    "\n",
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "      spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "num_epochs = 10\n",
    "num_iters = 10\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "use_pretrained = False\n",
    "\n",
    "if os.path.exists('./model/nmnist.pth') and use_pretrained:\n",
    "  net.load_state_dict(torch.load('./model/nmnist.pth'))\n",
    "  print('Model loaded')\n",
    "else:\n",
    "  # training loop\n",
    "  for epoch in range(num_epochs):\n",
    "      tqdm.write(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "      spk_list = []\n",
    "      target_list = []\n",
    "      for i, (data, targets) in tqdm(enumerate(iter(trainloader))):\n",
    "          data = data.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          net.train()\n",
    "          spk_rec = forward_pass(net, data)\n",
    "          loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "          # Gradient calculation + weight update\n",
    "          optimizer.zero_grad()\n",
    "          loss_val.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # Store loss history for future plotting\n",
    "          loss_hist.append(loss_val.item())\n",
    "\n",
    "          # print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "          # acc = SF.accuracy_rate(spk_rec, targets)\n",
    "          _, idx = spk_rec.sum(dim=0).max(1)\n",
    "          spk_list.extend(idx)\n",
    "          target_list.extend(targets)\n",
    "          # if i == num_iters:\n",
    "          #   break\n",
    "      spk_list = torch.stack(spk_list).tolist()\n",
    "      target_list = torch.stack(target_list).tolist()\n",
    "      accuracy = np.mean((spk_list == target_list).detach().cpu().numpy())\n",
    "      acc_hist.append(accuracy)\n",
    "      # print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "      tqdm.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "          # training loop breaks after 50 iterations\n",
    "          # if i == num_iters:\n",
    "          #   break\n",
    "\n",
    "torch.save(net.state_dict(), './model/nmnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc per class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred = spk_list\n",
    "target = target_list\n",
    "\n",
    "cm = confusion_matrix(target, pred)\n",
    "acc = accuracy_score(target, pred)\n",
    "\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "for i in range(10):\n",
    "  print(f'Accuracy for class {i}: {cm[i,i]/cm[i,:].sum()}')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Accuracy: {acc}')\n",
    "plt.savefig('./figures/nmnist_confusion_matrix.png')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(acc_hist)\n",
    "plt.title(\"Train Set Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data_dict = {}\n",
    "import torch\n",
    "for data, target in tqdm(trainloader):\n",
    "  if target.item() in data_dict:\n",
    "    continue\n",
    "  data = torch.tensor(data)\n",
    "  data = data.to(device)\n",
    "  data_dict[target.item()] = data\n",
    "#   break\n",
    "  if len(data_dict) == 10:\n",
    "    break\n",
    "  \n",
    "for k, v in data_dict.items():\n",
    "    fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
    "    labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    print(f\"The target label is: {labels[k]}\")\n",
    "    data = v.to(device)\n",
    "    spk_rec = forward_pass(net, data)\n",
    "    # print(spk_rec.shape)\n",
    "    # 创建一个空白图像，用于绘制动画的每一帧\n",
    "    fig.canvas.draw()\n",
    "    frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    # 创建一个可写的副本\n",
    "    frame_writable = frame.copy()\n",
    "\n",
    "    # 获取脉冲计数数据\n",
    "    spike_counts = spk_rec[:, idx].detach().cpu()\n",
    "    df = pd.DataFrame(spike_counts.numpy())\n",
    "    df.to_csv(f'spike_count_{k}.csv', index=False)\n",
    "    # 创建颜色映射\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    # 创建VideoWriter对象\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(f'spike_count_{k}.mp4', fourcc, 10.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # 绘制动画的每一帧\n",
    "    for t in range(len(spike_counts)):\n",
    "        # 清空图像\n",
    "        frame_writable.fill(0)\n",
    "\n",
    "        # 绘制条形图\n",
    "        for i, count in enumerate(spike_counts[t]):\n",
    "            color = cmap(i / len(labels))[:3]\n",
    "            color = list((np.array(color) * 255).astype(int))\n",
    "            # print(color)\n",
    "            color = [0, 255, 0]\n",
    "            cv2.rectangle(frame_writable, (i * 50, 0), ((i + 1) * 50, int(count * 100)),\n",
    "                        color, -1)\n",
    "            cv2.putText(frame_writable, labels[i], (i * 50 + 10, 120), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 将当前帧写入视频\n",
    "        out.write(frame_writable)\n",
    "\n",
    "    # 释放VideoWriter对象\n",
    "    out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
